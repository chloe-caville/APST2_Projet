{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "UuQIqRgJtkXn",
    "outputId": "e45f9369-2c5a-4dd0-ec1e-e21dd9c20869"
   },
   "outputs": [],
   "source": [
    "import keras as ks\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWn4PPpptkX0"
   },
   "source": [
    "# Pré-processing de textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COx4XiS3tkX1"
   },
   "source": [
    "Cette [page](https://keras.io/preprocessing/text/) détaille les méthodes de pré-processing de texte avec Keras et présente notamment la classe Tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ia2UwTetkX2"
   },
   "source": [
    "> Completer le code ci-dessous pour créer un analyseur lexical (tokenizer) avec keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zllsV--itkX3"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['je suis un étudiant de Nantes.',\n",
    "           'Je ne manque jamais les cours de machine learning!',\n",
    "           'je suis étudiant à Centrale', \n",
    "           'je suis jeune', \n",
    "           'je mange pasta']\n",
    "\n",
    "### ne conserver que 1000 mots dans le corpus :\n",
    "mon_tokenizer = Tokenizer(num_words=1000, \n",
    "                filters='\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n') \n",
    "\n",
    "mon_tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDRUwZ86tkX8"
   },
   "source": [
    "> Quel est l'index du mot \"machine\" dans cet encodage ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "S9UuWgdRtkX9",
    "outputId": "0bf380e3-8d7c-46cb-8ae1-bb187e26b45e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'je': 1, 'suis': 2, 'étudiant': 3, 'de': 4, 'un': 5, 'nantes': 6, 'ne': 7, 'manque': 8, 'jamais': 9, 'les': 10, 'cours': 11, 'machine': 12, 'learning!': 13, 'à': 14, 'centrale': 15, 'jeune': 16, 'mange': 17, 'pasta': 18}\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "print(mon_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNLZI-mFtkYE"
   },
   "source": [
    "> Afficher la liste des termes de ponctuations qui sont retirés par le Tokenizer. Modifier le fitre pour ne pas retirer le point d'exclamation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "4n2xQJJttkYH",
    "outputId": "075c7a92-8928-4f87-f4c9-6a5bf8acfec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TO DO ###\n",
    "print(mon_tokenizer.filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35kXP_2itkYP"
   },
   "source": [
    "> Transformer maintenant les mots en listes d'entiers avec la méthode `texts_to_sequences()` de la classe Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E6StTuVStkYQ",
    "outputId": "e75aeb58-59cb-452c-d897-abf81efd7d56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 5, 3, 4, 6], [1, 7, 8, 9, 10, 11, 4, 12, 13], [1, 2, 3, 14, 15], [1, 2, 16], [1, 17, 18]]\n"
     ]
    }
   ],
   "source": [
    "sequences = mon_tokenizer.texts_to_sequences(samples)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHFrtLqBtkYX"
   },
   "source": [
    "# Word Embeddings (plongement des mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7rcQuoztkYY"
   },
   "source": [
    "Il existe deux façons d'obtenir des embeddings de mots:\n",
    "\n",
    "- On peut apprendre un plongement pour une tache bien précise en amont (comme la classification des documents ou la prédiction des sentiments). Dans ce cas, on apprend le plongement comme on le fait pour un réseau de neurone classique.\n",
    "\n",
    "- On peut utiliser un embedding qui a été pré-entrainé pour une autre tâche, et que l'on \"recycle\" ici pour représenter les mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FDM9jQXtkYZ"
   },
   "source": [
    "### Apprentissage du plongement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpFEBhphtkYc"
   },
   "source": [
    "> En consultant la documentation sur la couche [`Embedding` de Keras](https://keras.io/layers/embeddings/), indiquer quels paramètres faut-il donner en argument à la fonction `Embedding` pour que celle-ci puisse représenter un plongement dans un espace de dimension 64 de séquences de longeur 10 mots dans corpus de 1000 mots retenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "DiMZrn9GtkYd",
    "outputId": "1f4aa5d7-43bd-4467-8089-cc7a9d4bc10d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(### TO DO ###)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_xO_EtwtkYf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXrINO1CtkYh"
   },
   "source": [
    "La couche `Embedding` prend en entrée un tenseur 2D d'entiers, de taille  `nombre de séquences` x  `longueur d'une séquence`.\n",
    "\n",
    "Toutes les séquences dans un bacth (de séquences) doivent donc avoir la même longueur, quitte à tronquer ou compléter avec des zeros les séquences trop longues ou trop courtes.\n",
    "\n",
    "Cette couche renvoie un tenseur 3D de valeurs numériques de taille `nombre de séquences` x  `longueur d'une séquence` x `dim d'arrivée du plongement`. \n",
    "\n",
    "Ces tenseurs 3D  peuvent ensuite être connectés à des couches récurrentes ou convolutionnelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgKde-WgtkYh"
   },
   "source": [
    "Dans un réseau de neurones, nous allons maintenant créer une première couche de plongement (embedding layer) et nous allons apprendre les poids de ce plongement exactement comme on le fait pour une couche dense.  Nous allons pour cela utiliser les données [imdb newswires Reuters](https://keras.io/datasets/#reuters-newswire-topics-classification) qui peuvent être directement chargées dans keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yKRnp7gMtkYi",
    "outputId": "4830d804-7b61-4633-dad5-3653c86d4970"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "max_mots = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XSpeleYtkYl"
   },
   "source": [
    "L'argument `num_words` correspond au nombre maximal de mots utilisés comme features. On le limite ici à 10000.\n",
    "\n",
    "> Vérifier que les mots ont été chargés sous la forme d'entiers. Que représente ici y ? Quel est l'objectif de ce problème d'apprentissage ? On parle \"d'analyse de sentiment\" (sentiment analysis ou opinion mining). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L3LuqLF3tkYl",
    "outputId": "072c5639-0872-4b72-c80a-bd7a5ab4d860"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYmkYuVrtkYo"
   },
   "source": [
    "> Bonus : retrouver les phrases à partir des vecteurs d'entiers (voir la doc de `imdb.load_dat`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTPAayRttkYo"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0W8nqgotkYs"
   },
   "source": [
    "> Utiliser la fonction [preprocessing.sequence.pad_sequences](https://keras.io/api/preprocessing/timeseries/#padsequences-function) pour transformer `x_train` et `x_test` en deux tenseurs 2D de tailles `nb de sequences` x  `long max d une sequence = 20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s0S7qsz-tkYs",
    "outputId": "1f0bd33d-cf99-4824-eda7-aa7ec1ec25e4"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raVGkng6tkYu"
   },
   "source": [
    "> Construire un réseau à propagation avant comme suit:\n",
    "- Une couche d'embedding qui plonge chaque mot dans un espace de dimension 8.\n",
    "- Une couche Flatten pour redimensionner le tenseur 3D des plongements en un tenseur 2D  de taille `nb de sequences` x  (8*20)  \n",
    "- Une couche dense avec activation sigmoid pour la classification finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "0Br819HBtkYv",
    "outputId": "50541480-1a21-4506-8bd1-e76adb38c9e8"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-RejtiAtkYx"
   },
   "source": [
    "> Utiliser un optimiseur `rmsprop` avec perte `binary_crossentropy` et suivi de la métrique `acc` (précision) le long de la trajectoire d'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "GZV3J0HHtkYy",
    "outputId": "6187d5de-f5c0-4b3a-ac34-059823397a45"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1C5JLdxtkY0"
   },
   "source": [
    "> Affichez le résumé du réseau de neurones ainsi construit et assurez-vous que vous comprenez les dimensions affichées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "h7jle7NftkY0",
    "outputId": "ce7d01d1-707b-440b-d746-f71faab4d5bb"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3bzGP3AtkY2"
   },
   "source": [
    "> Ajuster le modèle sur les données d'apprentissage et donner la précision de validation finale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "GABhvI-AtkY2",
    "outputId": "0a13e1a6-91c7-47e3-9109-a9ac944b53ff"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJhiFCHYtkY5"
   },
   "source": [
    "Le taux de bien classés tourne autour de 75%, ce qui est correct, mais on peut espérer faire mieux en utilisant le caractère \"séquentiel\" des phrases, grâce à des réseaux récurrents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlRRtQBftkY5"
   },
   "source": [
    "# Construction d'un réseau récurrent simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "339UmcyytkY6"
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9m0qT6d4tkY7"
   },
   "source": [
    "Une couche `SimpleRNN` prend en entrée un tenseur 3D de taille `batch_size` x `timesteps` (longeur de la séquence) x  `input_features` (typiquement la dimension de l'embedding). \n",
    "\n",
    "Comme tous les modèles récurrents, `SimpleRNN` peut renvoyer la suite complète de toutes les sorties pour chaque temps (le long de la séquence), ou bien tout simplement la denière sortie pour chaque séquence. \n",
    "\n",
    "> Expliquer la différence de dimension observée sur la couche récurrente dans les deux architectures proposées ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "zxzlwjjMtkY8",
    "outputId": "21751960-988e-4f4a-a686-7672e6466ea4"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = 1000,output_dim=32,input_length = 10))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "4e9H80STtkY9",
    "outputId": "c02abeeb-81cb-4871-bf88-38ae0f4407c0"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = 1000,output_dim=32,input_length = 10))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URasM4qDtkY_"
   },
   "source": [
    "> Préparer des données lexicales d'apprentissage et de test pour les données [`imdb`](https://keras.io/api/datasets/imdb/#load_data-function) selon les spécifications suivantes:\n",
    "- nombre de mots pris en compte : 10000 \n",
    "- longeur maximale des séquences : 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "L8R_jhZAtkY_",
    "outputId": "15be8d17-b515-4f86-b34b-f8a2651ca1f8"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwjsjJgdtkZB"
   },
   "source": [
    "> Construire un réseau à propagation avant comme suit:\n",
    "- Une couche d'embedding qui plonge chaque mot dans un espace de dimension 32.\n",
    "- Une couche `SimpleRNN` avec uniquement sortie finale\n",
    "- Une couche dense avec activation sigmoid pour la classification finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV0RW75ttkZC"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNn_c4LMtkZD"
   },
   "source": [
    "> Utiliser un optimiseur `rmsprop` avec perte `binary_crossentropy` et suivi de la métrique `acc` (précision) le long de la trajectoire d'optimisation. Ajuster le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cbUbqektkZE"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5W0vrlNGwBgg"
   },
   "source": [
    "# Construction d'un réseau récurrent avec cellules LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_dK3HRtwhak"
   },
   "source": [
    "> Construire enfin un réseau similaire où vous aurez remplacé la couche SimpleRNN par une couche [LSTM](https://keras.io/layers/recurrent/#lstm).\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "6nSBWWW2wgv1",
    "outputId": "e7d7eeb6-09d9-4d17-fc4c-315807b9dffe"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06AUej2ttkZU"
   },
   "source": [
    "# Utilisation d'un embedding pré-entrainé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DI1PFMdtkZV"
   },
   "source": [
    "Cette fois nous allons partir des données `Imdb` brutes et plonger celles-ci dans un espace via un plongement qui a déjà été ajusté (sur des données différentes et pour un problème autre). Nous allons utiliser [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove) : télécharger glove.6B.zip (près d'un giga !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Wa3SLdDtkZV"
   },
   "source": [
    "> Télécharger les données brutes à [cette adresse](http://mng.bz/0tIo). Les textes positifs et négatifs sont classés dans des repertoires de même nom. Compléter le code ci-dessous pour importer et préparer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tmklq8VptkZV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = ### TO DO ###\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in [### TO DO ###]:\n",
    "    dir_name = os.path.join(train_rep, ### TO DO ###)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, ### TO DO ###))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == ### TO DO ###:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJFHlNCItkZX",
    "outputId": "59fa292a-4360-490d-95c0-773eb3d9b566"
   },
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Px8rgO_TtkZa"
   },
   "source": [
    "> Effectuer les opérations de traitement lexical (tokenization) pour un corpus de 10000 mots et des séquence de mots d'une longueur maximale de 100 mots. Transformer `labels` en un vecteur `numpy`. Vérifier les dimensions des objets construits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rZyddzitkZt",
    "outputId": "2b8cc3f2-6a54-438b-f407-4d055b24f9e3"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM1r5wmctkZw"
   },
   "source": [
    "> Extraire 1000 données pour l'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FH37Eu7tkZw"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeltkAkhtkZx"
   },
   "source": [
    "> En suivant la documentation de Keras sur cette [page](https://keras.io/examples/nlp/pretrained_word_embeddings/), utiliser  un embedding de type Glove sur les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUdFEwuStkZy"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwVzJ3NptkZ0"
   },
   "source": [
    "> Utiliser cet embedding pour construire des réseaux recurrents ou non pour prédire la sortie Y. Evaluer la précision de vos modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8lsAZQVtkZ0"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDRxcABAtkZ5"
   },
   "source": [
    "> Utiliser cet embedding de mots pour évaluer la proximité entre quelques phrases que vous choisirez. Vous pourrez représenter les données dans le premier plan factoriel d'une ACP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRzZUbSptkZ6"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN-NLP_correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7673fce31fe4afe50b4d7357ba4e3f000f877c9b2250741ca11e5d63029f695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
